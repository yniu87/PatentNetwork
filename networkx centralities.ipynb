{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating dataframe from csv file\n",
    "cumsum = pd.read_csv('../Citation_data/citation_cumsum.csv')\n",
    "cumsum.drop(columns=['cited_subcategory_id'], inplace=True)\n",
    "cumsum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionary of node weights\n",
    "node_weights = dict()\n",
    "row_number = 0\n",
    "for node in list(cumsum.columns):\n",
    "    node_weights[node] = cumsum.at[row_number, node]\n",
    "    row_number += 1\n",
    "print(node_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create graph and add nodes\n",
    "G = nx.MultiDiGraph()\n",
    "G.clear()\n",
    "for node in list(cumsum.columns):\n",
    "    G.add_node(node, weight=node_weights[node])\n",
    "list(G.nodes(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add edges with weights\n",
    "row_number = 0\n",
    "for node_1 in list(cumsum.columns):\n",
    "    for node_2 in list(cumsum.columns):\n",
    "        if str(node_1) != str(node_2):\n",
    "            G.add_edge(node_1, node_2, weight=cumsum.at[row_number, node_2])\n",
    "    row_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Customize node and edge sizes according to size of internal and external linkage\n",
    "row_number = 0\n",
    "size_list = []\n",
    "edge_list = []\n",
    "for node in list(cumsum.columns):\n",
    "    size_list.append(abs((cumsum.at[row_number, node])**(1/1.8)) + 100)\n",
    "    row_number += 1\n",
    "print(size_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate betweenness centrality\n",
    "bc = nx.betweenness_centrality(G, weight='node_weights') #is this weight right\n",
    "evc = nx.eigenvector_centrality_numpy(G, weight='node_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in data\n",
    "string = '../Citation_data/5_year_citation_snapshot' \n",
    "csv = '.csv'\n",
    "year = np.array([1980, 1985, 1990, 1995, 2000, 2005, 2010])\n",
    "\n",
    "d = {}\n",
    "for y in year:\n",
    "    fName = string + str(y) + csv \n",
    "    \n",
    "    #Create a dataframe\n",
    "    temp = pd.read_csv(fName)\n",
    "    temp.drop(columns=['cited_subcategory_id'], inplace=True)\n",
    "    \n",
    "    #Create dictionary of node weights\n",
    "    node_weights = dict()\n",
    "    row_number = 0\n",
    "    for node in list(temp.columns):\n",
    "        node_weights[node] = temp.at[row_number, node]\n",
    "        row_number += 1\n",
    "    \n",
    "    #Create graph and add nodes\n",
    "    G = nx.MultiDiGraph()\n",
    "    G.clear()\n",
    "    for node in list(temp.columns):\n",
    "        G.add_node(node, weight=node_weights[node])\n",
    "    list(G.nodes(data=True))\n",
    "    \n",
    "    #Add edges with weights\n",
    "    row_number = 0\n",
    "    for node_1 in list(temp.columns):\n",
    "        for node_2 in list(temp.columns):\n",
    "            if str(node_1) != str(node_2):\n",
    "                G.add_edge(node_1, node_2, weight=temp.at[row_number, node_2])\n",
    "        row_number += 1\n",
    "    \n",
    "    #Customize node and edge sizes according to size of internal and external linkage\n",
    "    row_number = 0\n",
    "    size_list = []\n",
    "    edge_list = []\n",
    "    for node in list(temp.columns):\n",
    "        size_list.append(abs((temp.at[row_number, node])**(1/1.8)) + 100)\n",
    "        row_number += 1\n",
    "    \n",
    "    #Calculate betweenness centrality\n",
    "    name = 'bc' + str(y)\n",
    "    d[name] = nx.betweenness_centrality(G, weight='node_weights') #is this weight correct\n",
    "    name = 'evc' + str(y)\n",
    "    d[name] = nx.eigenvector_centrality_numpy(G, weight='node_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if weights were properly added, if they are, this should be false\n",
    "d['bc1980']==d['bc1985']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
